{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nice_layers' from '/home/davidclark/Projects/VectorizedNets/nice_layers.py'>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from imp import reload\n",
    "import nice_layers as vnn\n",
    "reload(vnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make model\n",
    "\n",
    "reload(vnn)\n",
    "\n",
    "nonneg = True\n",
    "model = nn.Sequential(vnn.Conv2d(10, 30, 64, 3, nonneg=False, expanded_input=True),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.Conv2d(10, 64, 64, 3, nonneg=nonneg),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.AvgPool2d(2),\n",
    "                      vnn.Conv2d(10, 64, 64, 3, nonneg=nonneg),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.Conv2d(10, 64, 128, 3, nonneg=nonneg),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.AvgPool2d(2),\n",
    "                      vnn.Conv2d(10, 128, 256, 3, nonneg=nonneg),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.Conv2d(10, 256, 256, 3, nonneg=nonneg),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.Flatten(),\n",
    "                      vnn.Linear(10, 256, 256, nonneg=nonneg),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.Linear(10, 256, 256, nonneg=nonneg),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.Linear(10, 256, 1, nonneg=nonneg))\n",
    "\n",
    "model = model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make model\n",
    "\n",
    "reload(vnn)\n",
    "\n",
    "nonneg = True\n",
    "model = nn.Sequential(vnn.Conv2d(10, 30, 96, 2, stride=2, nonneg=False, expanded_input=True),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.Conv2d(10, 96, 96, 3, stride=2, nonneg=nonneg, expanded_input=True),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.Conv2d(10, 96, 96, 3, stride=2, nonneg=nonneg, expanded_input=True),\n",
    "                      vnn.tReLU(),\n",
    "                      vnn.Flatten(),\n",
    "                      vnn.Linear(10, 864, 1, nonneg=nonneg))\n",
    "\n",
    "model = model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated t with shape (10, 96, 16, 16)\n",
      "Instantiated t with shape (10, 96, 7, 7)\n",
      "Instantiated t with shape (10, 96, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10, 1])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input = expand_input(torch.randn(5, 3, 32, 32), 10).to(0)\n",
    "out = model(input) #[:7](input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_conv_l0(weight):\n",
    "    out_channels, in_channels = weight.shape[:2]\n",
    "    W_shape = weight.shape[1:]\n",
    "    for i in range(out_channels // 2):\n",
    "        W = torch.rand(W_shape)*2. - 1.\n",
    "        weight[2*i] = W\n",
    "        weight[2*i + 1] = -W\n",
    "\n",
    "def init_conv(weight):\n",
    "    out_channels, in_channels = weight.shape[:2]\n",
    "    W_shape = weight.shape[2:]\n",
    "    for i in range(out_channels // 2):\n",
    "        for j in range(in_channels // 2):\n",
    "            W = torch.rand(W_shape)*2. - 1.\n",
    "            i1, i2 = i*2, i*2 + 1\n",
    "            j1, j2 = j*2, j*2 + 1\n",
    "            #if np.random.rand() < 0.5:\n",
    "            weight[i1, j1] = F.relu(W)\n",
    "            weight[i2, j2] = F.relu(W)\n",
    "            #else:\n",
    "            weight[i1, j2] = F.relu(-W)\n",
    "            weight[i2, j1] = F.relu(-W)\n",
    "                \n",
    "def init_t(t):\n",
    "    features = t.shape[1]\n",
    "    for i in range(features // 2):\n",
    "        t[:, 2*i + 1] = -t[:, 2*i]\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k1 = 3*4\n",
    "k2 = 96*9/2.\n",
    "k3 = 96*9/2.\n",
    "\n",
    "with torch.no_grad():\n",
    "    init_conv_l0(model[0].conv.weight)\n",
    "    model[0].conv.weight *= np.sqrt(1./k1)\n",
    "    init_t(model[1].t)\n",
    "    init_conv(model[2].conv.weight)\n",
    "    model[2].conv.weight *= np.sqrt(1./k2)\n",
    "    init_t(model[3].t)\n",
    "    init_conv(model[4].conv.weight)\n",
    "    model[4].conv.weight *= np.sqrt(1./k3)\n",
    "    init_t(model[5].t)\n",
    "\n",
    "    \n",
    "#model = model.to(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model[0].conv.weight *= 4\n",
    "    model[2].conv.weight *= 4\n",
    "    model[4].conv.weight *= 2\n",
    "model = model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = expand_input(torch.randn(5, 3, 32, 32), 10).to(0)\n",
    "plt.hist(model[:](input).cpu().detach().numpy().flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model[5].conv.weight.cpu().detach().numpy().flatten(), bins=100)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update utilty\n",
    "def set_model_grads(model, output, labels):\n",
    "    targets = torch.eye(10, device=labels.device)[labels]\n",
    "    output_error = F.softmax(output, dim=1) - targets\n",
    "    for i in range(len(model)):\n",
    "        layer = model[i]\n",
    "        if type(layer) in (vnn.Conv2d, vnn.Linear):\n",
    "            if (i == len(model) - 1) or (type(model[i+1]) not in (vnn.ReLU, vnn.tReLU)):\n",
    "                mask = torch.ones(layer.mask_shape, device=output.device)\n",
    "            else:\n",
    "                mask = model[i+1].mask\n",
    "            layer.set_grad(mask, output_error)\n",
    "        \n",
    "#input utility\n",
    "def expand_input(input, category_dim):\n",
    "    #input = (batch, channels, width, height)\n",
    "    batch_size, in_channels = input.shape[:2]\n",
    "    expanded_input = torch.zeros((batch_size, category_dim, in_channels*category_dim) + input.shape[2:])\n",
    "    for i in range(category_dim):\n",
    "        expanded_input[:, i, i*in_channels:(i+1)*in_channels] = input\n",
    "    return expanded_input\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#load cifar-10\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = torchvision.datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Instantiated t with shape (10, 32, 30, 30)\n",
      "Instantiated t with shape (10, 64, 28, 28)\n",
      "Instantiated t with shape (10, 128, 12, 12)\n",
      "Instantiated t with shape (10, 128, 10, 10)\n",
      "Instantiated t with shape (10, 256, 3, 3)\n",
      "Instantiated t with shape (10, 256, 1, 1)\n",
      "Instantiated t with shape (10, 1024)\n",
      "Instantiated t with shape (10, 512)\n",
      "0.10054\n",
      "epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-bc4631ce9080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#print(model[3].conv.weight)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(\"batch\", batch_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0)\n",
    "\n",
    "for epoch_idx in range(1000):\n",
    "    print(\"epoch\", epoch_idx)\n",
    "    epoch_loss = 0.\n",
    "    epoch_correct = 0.\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        nans = torch.any(torch.isnan(model[2].conv.weight)).item()\n",
    "        if nans:\n",
    "            print(\"NAN!\")\n",
    "        #print(model[3].conv.weight)\n",
    "        #print(\"batch\", batch_idx)\n",
    "        input = expand_input(data, 10).to(0)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)[..., 0]\n",
    "        #loss = loss_fn(output.detach(), labels)\n",
    "        #epoch_loss += loss.item()\n",
    "        correct = (output.detach().cpu().argmax(dim=1) == labels).float().sum().item()\n",
    "        #print(correct / 128.)\n",
    "        epoch_correct += correct\n",
    "        \n",
    "        set_model_grads(model, output, labels.to(0))\n",
    "        #print(model[0].conv.weight.grad)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), .1)\n",
    "        optimizer.step()\n",
    "        #for layer in model:\n",
    "        #    layer.post_step_callback()\n",
    "            \n",
    "\n",
    "    print(epoch_correct / 50000.)\n",
    "    #print(epoch_loss / (batch_idx + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "correct 0.26804\n",
      "loss 2.059814945206313\n",
      "1\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "correct 0.32188\n",
      "loss 1.889460609087249\n",
      "2\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "correct 0.34726\n",
      "loss 1.8273332948270051\n",
      "3\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "correct 0.35988\n",
      "loss 1.7867720560039706\n",
      "4\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "correct 0.37438\n",
      "loss 1.757153989408937\n",
      "5\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-3372edfe71e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepoch_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neuro/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neuro/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neuro/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neuro/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neuro/lib/python3.8/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neuro/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neuro/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neuro/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'std evaluated to zero after conversion to {}, leading to division by zero.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch_idx in range(1000):\n",
    "    print(epoch_idx)\n",
    "    epoch_loss = 0.\n",
    "    epoch_correct = 0.\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(batch_idx)\n",
    "        input = expand_input(data, 10).to(0)\n",
    "        out = model(input)[..., 0]\n",
    "        loss = loss_fn(out, labels.to(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for layer in model[1:]: layer.post_step_callback()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += (out.detach().cpu().argmax(dim=1) == labels).float().sum().item()\n",
    "    print(\"correct\", epoch_correct / 50000.)\n",
    "    print(\"loss\", epoch_loss / (batch_idx + 1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4966)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch_correct = 0.\n",
    "for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "    input = expand_input(data, 10).to(0)\n",
    "    out = model(input)[..., 0]\n",
    "    epoch_correct += (out.detach().cpu().argmax(dim=1) == labels).float().sum()\n",
    "\n",
    "print(epoch_correct / 10000.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated t with shape (10, 32, 30, 30)\n",
      "Instantiated t with shape (10, 64, 28, 28)\n",
      "Instantiated t with shape (10, 128, 12, 12)\n",
      "Instantiated t with shape (10, 128, 10, 10)\n",
      "Instantiated t with shape (10, 256, 3, 3)\n",
      "Instantiated t with shape (10, 256, 1, 1)\n",
      "Instantiated t with shape (10, 1024)\n",
      "Instantiated t with shape (10, 512)\n"
     ]
    }
   ],
   "source": [
    "out = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5015921.5000],\n",
       "         [ 3048779.0000],\n",
       "         [ 2860563.0000],\n",
       "         ...,\n",
       "         [  904737.7500],\n",
       "         [ 3212645.5000],\n",
       "         [-2308516.7500]],\n",
       "\n",
       "        [[-2976155.5000],\n",
       "         [-2003627.6250],\n",
       "         [-1668159.3750],\n",
       "         ...,\n",
       "         [ -301107.5000],\n",
       "         [-1824721.1250],\n",
       "         [  458257.1562]],\n",
       "\n",
       "        [[  -56928.6328],\n",
       "         [ -421825.1250],\n",
       "         [ -220883.4688],\n",
       "         ...,\n",
       "         [ -646944.3750],\n",
       "         [-1433247.7500],\n",
       "         [  810331.3750]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 6916157.0000],\n",
       "         [ 1867002.8750],\n",
       "         [ 2697788.2500],\n",
       "         ...,\n",
       "         [  413962.9688],\n",
       "         [ 2753534.5000],\n",
       "         [ 1709518.6250]],\n",
       "\n",
       "        [[ 1949936.5000],\n",
       "         [ -515905.3438],\n",
       "         [  137847.6406],\n",
       "         ...,\n",
       "         [   55038.9023],\n",
       "         [ -103463.7188],\n",
       "         [ 1440541.7500]],\n",
       "\n",
       "        [[-2459864.0000],\n",
       "         [  615623.3125],\n",
       "         [-1055614.1250],\n",
       "         ...,\n",
       "         [ -136023.7500],\n",
       "         [ 4870996.5000],\n",
       "         [-4825188.5000]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
